{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\" Takes in a string, returns a list words in the string that aren't stopwords\n",
    "    Parameters:\n",
    "        tweet (string):  string of text to be tokenized\n",
    "    Returns:\n",
    "        stopwords_removed (list): list of all words in tweet, not including stopwords\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token not in stopwords_list]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized(series):\n",
    "    \"\"\" Takes in a series containing strings or lists of strings, and creates a single list of all the words\n",
    "    Parameters:\n",
    "        series (series): series of text in the form of strings or lists of string \n",
    "\n",
    "    Returns:\n",
    "        tokens (list): list of every word in the series, not including stopwords\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = ' '.join([tweet.lower() if type(tweet)==str else ' '.join([tag.lower() for tag in tweet]) for tweet in series])\n",
    "    tokens = process_tweet(corpus)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordfrequency(series, top):\n",
    "    \"\"\" Returns the frequency of words in a list of strings.\n",
    "    Parameters:\n",
    "        series (iterable): List of strings to be combined and analyzed\n",
    "        top (int): The number of top words to return.\n",
    "    Returns:\n",
    "        list (tuples): List of word and value pairs for the top words in the series.\n",
    "    \"\"\"\n",
    "    frequencies = FreqDist(tokenized(series))\n",
    "    return frequencies.most_common(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(series, *top):\n",
    "    \"\"\" Take in a list of lists and create a WordCloud visualization for those terms.\n",
    "    Parameters:\n",
    "            series (iterable): A list of lists containing strings.\n",
    "    Returns:\n",
    "        None: The ouput is a visualization of the strings in series in terms of the\n",
    "            frequency of their occurrence.\n",
    "    \"\"\"\n",
    "    # if top[0]:\n",
    "    # \tseries=wordfrequency(series,top[0])\n",
    "    vocab = tokenized(series)\n",
    "    if not top[0]:\n",
    "        top[0]=200\n",
    "    cloud=WordCloud(max_words=top[0]).generate(' '.join([word for word in vocab]))\n",
    "    plt.imshow(cloud,interpolation='bilinear')\n",
    "    plt.plot(figsize = (8,4))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_conda",
   "language": "python",
   "name": "sentiment_conda"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
